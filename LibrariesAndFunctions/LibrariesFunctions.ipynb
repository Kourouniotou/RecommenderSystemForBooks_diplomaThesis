{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tasos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tasos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tasos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tasos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.express as px\n",
    "import pyLDAvis.gensim_models\n",
    "# NLTK Stop words\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "# Visualize the topics\n",
    "#import pyLDAvis.gensim_models\n",
    "import pickle \n",
    "#import pyLDAvis\n",
    "import numpy as np\n",
    "import ast\n",
    "from scipy.stats import entropy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np\n",
    "from nltk import pos_tag\n",
    "import ast\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from gensim.models import TfidfModel\n",
    "import pandas as pd \n",
    "import re \n",
    "import time \n",
    "import nltk \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords') \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize \n",
    "import string \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial import distance\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import coherencemodel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import itertools\n",
    "from statistics import mean\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import coherencemodel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import collections\n",
    "from datetime import datetime\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "nltk.download('stopwords') # run this one time\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# libraries for visualization\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, DetectorFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "#sentence to words\n",
    "def sent2Words(text):\n",
    "    return ' '.join([text]).split()\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "#list of strings to string\n",
    "def listOfStingsToString(list_):\n",
    "    return (' '.join(list_))\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "#remove stopwords \n",
    "stopWords = stopwords.words('english')\n",
    "stopWords.extend(['from', 'subject', 'ww', 'www', 'com', 're', 'edu', 'use', 'go', 'book', 'author', 'writer', 'poem', 'one', 'new', 'write', 'refer', 'tell', 'story', 'reader'])\n",
    "stopWords = set(stopWords)\n",
    "\n",
    "def removeStopwords(text):\n",
    "    noStopWords = [word for word in word_tokenize(text) if word not in stopWords]\n",
    "    \n",
    "    words_string = [word for word in noStopWords if len(word)>2 or word.isdigit()]\n",
    "    return words_string\n",
    "\n",
    "###############################################################################################\n",
    "# count words in list\n",
    "def countWords(text):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# Function that replace 'n' with 'and'\n",
    "def replacenWithAnd(text):\n",
    "    return([word if word != 'n' else 'and' for word in text])\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "#function for lemmatization. Lemmatize all speech tags that starts with J,V,N,R to adejctives, verbs, nouns and adverb\n",
    "def lemmatizationAll(text, flag):\n",
    "    lemmaWords = []\n",
    "    \n",
    "    if flag == 0:\n",
    "        range_ = ast.literal_eval(text)\n",
    "    elif flag == 1:\n",
    "        range_ = text\n",
    "        \n",
    "    for word, tag in pos_tag(range_):\n",
    "        if tag.startswith('J'):\n",
    "            lemmaWords.append(lemmatizer.lemmatize(word, pos='a'))\n",
    "        elif tag.startswith('V'):\n",
    "            lemmaWords.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('N'):\n",
    "            lemmaWords.append(lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('R'):\n",
    "            lemmaWords.append(lemmatizer.lemmatize(word, pos='r'))\n",
    "        else:\n",
    "            lemmaWords.append(lemmatizer.lemmatize(word, pos='n'))\n",
    "    return(lemmaWords)\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "#Function that keep only the nouns from the already lemmatized data\n",
    "def lemmatizationNouns(text):\n",
    "    posDesc = nltk.pos_tag(text)\n",
    "    lemmaWords = [word[0] for word in posDesc if (word[1].startswith('N')) and (word[1] not in bigrams) and (word[1] not in trigrams)]\n",
    "    return lemmaWords\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "# Function that keep only the nouns and verbs from the already lemmatized data\n",
    "def lemmatizationNounsAndVerbs(text):\n",
    "    posDesc = nltk.pos_tag(text)\n",
    "    lemmaWords = [word[0] for word in posDesc if (word[1].startswith('N') or word[1].startswith('V')) and (word[1] not in bigrams) and (word[1] not in trigrams)]\n",
    "    return lemmaWords\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "#Function that remove Authors names from description\n",
    "def removeAuthorsNames(text):\n",
    "    for name in authorsNames[\"Authors\"]:\n",
    "        text = text.replace(name, '')\n",
    "    return text\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "# Function that converts all uppercase letters to lowercase\n",
    "def lowerCasing(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "# Function that removes all the special characters/symbols\n",
    "def removeSpecialCharacters(text):\n",
    "    cleanText = re.sub(r'[^\\w\\s]', ' ', text) \n",
    "    cleanText = re.sub(r'[\\W_]', ' ', cleanText)\n",
    "    return cleanText\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "# Function that removes extra whitespaces between words\n",
    "def removeWhitespace(text):\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    cleanText = re.sub(pattern, ' ', text)\n",
    "    cleanText = cleanText.replace('?', ' ? ').replace(')', ') ')\n",
    "    return cleanText\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "#Function that removes numbers from text\n",
    "def removeNumbers(text):\n",
    "    cleanText = re.sub(r'\\d+', ' ', text) \n",
    "    return cleanText\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "#Function that converts \"['play', 'run']\" to ['play', 'run']\n",
    "def strToListOfStrs(text):\n",
    "    return(ast.literal_eval(text))\n",
    "\n",
    "\n",
    "#Function that removes new line from text\n",
    "def removeNewlines(text):\n",
    "    cleanText = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return cleanText\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "#Function that removes html tags from text\n",
    "def removeHtmlTags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleanText = soup.get_text(separator=\" \")\n",
    "    return cleanText\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "#Function that removes links from text\n",
    "def removeLinks(text):\n",
    "    cleanText = re.sub(r'http\\S+', '', text)\n",
    "    cleanText = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", cleanText)\n",
    "    return cleanText\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "#Function that removes repetition of characters in text\n",
    "def reducingIncorrectCharacterRepeatation(text):\n",
    "    firstPatter = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "    cleanText = firstPatter.sub(r\"\\1\\1\", text) \n",
    "    pattern = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "    combinedFormatted = pattern.sub(r'\\1', cleanText)\n",
    "    cleanText = re.sub(' {2,}',' ', combinedFormatted)\n",
    "    return cleanText\n",
    "\n",
    "\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"i would\",\"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\",\n",
    "}\n",
    "\n",
    "#################################################################################################\n",
    "#Function that expand contraction words\n",
    "def expandContractions(text, contraction_mapping =  CONTRACTION_MAP):\n",
    "    \n",
    "    list_Of_tokens = text.split(' ')\n",
    "    \n",
    "    for Word in list_Of_tokens: \n",
    "         if Word in CONTRACTION_MAP: \n",
    "                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n",
    "    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
    "    return String_Of_tokens\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "# Function that does the exact opposite job of tokenization\n",
    "def listOfStingsToString(list_):\n",
    "    return (' '.join(list_))\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "# Function that sorts tuples\n",
    "def sortTuple(tup):  \n",
    "    return(sorted(tup, key = lambda x: x[1], reverse = True))  \n",
    "\n",
    "#################################################################################################\n",
    "#Function that convert a 2D list to 1D list\n",
    "def oneDList(text):\n",
    "    return(list(chain.from_iterable(text)))\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "# Concatenate n-grams\n",
    "def replaceNgram(text, flag):\n",
    "    for gram in bigrams.Bigrams:\n",
    "        text = text.replace(gram, '_'.join(gram.split()))\n",
    "    if flag == 1:\n",
    "        for gram in trigrams.Trigrams:\n",
    "            text = text.replace(gram, '_'.join(gram.split()))\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
